{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d59b7b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers: 60\n"
     ]
    }
   ],
   "source": [
    "# H1) 설정 & 매니페스트 로드\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "CSR_DIR = Path(\"/home/yyb02274/yolov5/csr_dump_ratio[0.9]\")\n",
    "WS_DIR = Path(\"/home/yyb02274/yolov5/ws_dump_ratio[0.9]\")\n",
    "HUF_DIR = Path(\"/home/yyb02274/yolov5/Deep_Compression_ratio[0.9]/huff_dump_ratio[0.9]\")\n",
    "HUF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# (옵션) row_ptr도 델타 허프만 압축할지 여부\n",
    "COMPRESS_ROW_PTR = True\n",
    "\n",
    "mani   = json.loads((CSR_DIR / \"manifest_ratio[0.9].json\").read_text())\n",
    "mani_ws= json.loads((WS_DIR / \"manifest_ws.json\").read_text())\n",
    "print(\"layers:\", len(mani[\"layers\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1836fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H2) 델타 인코딩 유틸\n",
    "import numpy as np\n",
    "\n",
    "def col_ind_to_deltas_per_row(row_ptr: np.ndarray, col_ind: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"각 row마다 prev=0으로 시작해 col 인덱스의 델타를 기록\"\"\"\n",
    "    deltas = np.empty_like(col_ind)\n",
    "    w = 0\n",
    "    for r in range(len(row_ptr)-1):\n",
    "        s, e = int(row_ptr[r]), int(row_ptr[r+1])\n",
    "        prev = 0\n",
    "        for i in range(s, e):\n",
    "            cur = int(col_ind[i])\n",
    "            deltas[w] = cur - prev\n",
    "            prev = cur\n",
    "            w += 1\n",
    "    return deltas\n",
    "\n",
    "def row_ptr_deltas(row_ptr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"row_ptr 델타 = 각 row의 nnz (첫 항 0 제외)\"\"\"\n",
    "    return np.diff(row_ptr).astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2b1c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H3) 허프만 코어\n",
    "import heapq\n",
    "from collections import Counter\n",
    "\n",
    "def huffman_code_lengths(freq: dict[int,int]) -> dict[int,int]:\n",
    "    \"\"\"심볼->빈도 -> 코드길이. 빈도 0 제외. 단일 심볼은 길이=1.\"\"\"\n",
    "    items = [(f, s) for s, f in freq.items() if f > 0]\n",
    "    if not items:\n",
    "        return {}\n",
    "    if len(items) == 1:\n",
    "        return {items[0][1]: 1}\n",
    "\n",
    "    heap = [[f, [s]] for f, s in items]\n",
    "    heapq.heapify(heap)\n",
    "    lengths = {s:0 for _, s in items}\n",
    "    while len(heap) > 1:\n",
    "        f1, syms1 = heapq.heappop(heap)\n",
    "        f2, syms2 = heapq.heappop(heap)\n",
    "        for s in syms1: lengths[s] += 1\n",
    "        for s in syms2: lengths[s] += 1\n",
    "        heapq.heappush(heap, [f1+f2, syms1+syms2])\n",
    "    return lengths\n",
    "\n",
    "def canonical_codes_from_lengths(lengths: dict[int,int]) -> dict[int,str]:\n",
    "    \"\"\"심볼->길이 -> 캐논컬 허프만 코드(비트문자열).\"\"\"\n",
    "    if not lengths:\n",
    "        return {}\n",
    "    pairs = sorted((l, s) for s, l in lengths.items())  # 길이, 심볼\n",
    "    codes = {}\n",
    "    code = 0\n",
    "    prev_len = pairs[0][0]\n",
    "    for l, s in pairs:\n",
    "        if l > prev_len:\n",
    "            code <<= (l - prev_len)\n",
    "            prev_len = l\n",
    "        codes[s] = format(code, f\"0{l}b\")\n",
    "        code += 1\n",
    "    return codes\n",
    "\n",
    "class BitWriter:\n",
    "    def __init__(self):\n",
    "        self.buf = bytearray()\n",
    "        self.bitbuf = 0\n",
    "        self.nbits = 0\n",
    "        self.total_bits = 0\n",
    "    def write_bits(self, bits: str):\n",
    "        self.total_bits += len(bits)\n",
    "        for b in bits:\n",
    "            self.bitbuf = (self.bitbuf << 1) | (1 if b == '1' else 0)\n",
    "            self.nbits += 1\n",
    "            if self.nbits == 8:\n",
    "                self.buf.append(self.bitbuf)\n",
    "                self.bitbuf = 0\n",
    "                self.nbits = 0\n",
    "    def finish(self) -> bytes:\n",
    "        if self.nbits:\n",
    "            self.bitbuf <<= (8 - self.nbits)\n",
    "            self.buf.append(self.bitbuf)\n",
    "            self.bitbuf = 0\n",
    "            self.nbits = 0\n",
    "        return bytes(self.buf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b50100eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Huffman encoding done: /home/yyb02274/yolov5/Deep_Compression_ratio[0.9]/huff_dump_ratio[0.9]\n"
     ]
    }
   ],
   "source": [
    "# H4) 허프만 인코딩 실행 & 저장\n",
    "import json, numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def load_layer_arrays(layer_name):\n",
    "    csr = np.load(CSR_DIR / f\"{layer_name.replace('.', '_')}.npz\")\n",
    "    ws  = np.load(WS_DIR  / f\"{layer_name.replace('.', '_')}.npz\")\n",
    "    return (csr[\"row_ptr\"].astype(np.int64),\n",
    "            csr[\"col_ind\"].astype(np.int64),\n",
    "            ws[\"codebook\"].astype(np.float32),\n",
    "            ws[\"indices\"].astype(np.int64))\n",
    "\n",
    "summary = []\n",
    "tot_idx_bits = tot_col_bits = tot_rpd_bits = 0\n",
    "tot_idx_bytes = tot_col_bytes = tot_rpd_bytes = 0\n",
    "tot_json_bytes = 0\n",
    "\n",
    "for L in mani[\"layers\"]:\n",
    "    name   = L[\"name\"]\n",
    "    row_ptr, col_ind, codebook, indices = load_layer_arrays(name)\n",
    "\n",
    "    # A) indices 스트림 허프만\n",
    "    freq_idx = Counter(indices.tolist()) if indices.size else Counter()\n",
    "    len_idx  = huffman_code_lengths(freq_idx)\n",
    "    codes_idx= canonical_codes_from_lengths(len_idx)\n",
    "    bw = BitWriter()\n",
    "    for s in indices:\n",
    "        bw.write_bits(codes_idx[int(s)])  # 단일심볼인 경우 코드 길이 1 할당됨\n",
    "    idx_bytes = bw.finish()\n",
    "    idx_bits  = bw.total_bits\n",
    "    (HUF_DIR / f\"{name.replace('.', '_')}_idx.bin\").write_bytes(idx_bytes)\n",
    "\n",
    "    # B) col_ind 델타(행 단위 초기화) 허프만\n",
    "    deltas = col_ind_to_deltas_per_row(row_ptr, col_ind)\n",
    "    freq_col = Counter(deltas.tolist()) if deltas.size else Counter()\n",
    "    len_col  = huffman_code_lengths(freq_col)\n",
    "    codes_col= canonical_codes_from_lengths(len_col)\n",
    "    bw = BitWriter()\n",
    "    for s in deltas:\n",
    "        bw.write_bits(codes_col[int(s)])\n",
    "    col_bytes = bw.finish()\n",
    "    col_bits  = bw.total_bits\n",
    "    (HUF_DIR / f\"{name.replace('.', '_')}_col.bin\").write_bytes(col_bytes)\n",
    "\n",
    "    # C) row_ptr 델타(옵션)\n",
    "    rpd_bits = 0\n",
    "    rpd_bytes = 0\n",
    "    meta_rpd = None\n",
    "    if COMPRESS_ROW_PTR:\n",
    "        rpd = row_ptr_deltas(row_ptr)\n",
    "        freq_rpd = Counter(rpd.tolist()) if rpd.size else Counter()\n",
    "        len_rpd  = huffman_code_lengths(freq_rpd)\n",
    "        codes_rpd= canonical_codes_from_lengths(len_rpd)\n",
    "        bw = BitWriter()\n",
    "        for s in rpd:\n",
    "            bw.write_bits(codes_rpd[int(s)])\n",
    "        rpd_bytes = bw.finish()\n",
    "        rpd_bits  = bw.total_bits\n",
    "        (HUF_DIR / f\"{name.replace('.', '_')}_rpd.bin\").write_bytes(rpd_bytes)\n",
    "        meta_rpd = {\n",
    "            \"stream\": \"rowptr_delta\",\n",
    "            \"n_symbols\": int(rpd.size),\n",
    "            \"code_lengths\": {str(int(k)): int(v) for k, v in len_rpd.items()},\n",
    "            \"bit_length\": int(rpd_bits)\n",
    "        }\n",
    "\n",
    "    # per-layer json (코드길이/비트길이 메타)\n",
    "    layer_meta = {\n",
    "        \"name\": name,\n",
    "        \"shape2d\": L[\"shape2d\"],\n",
    "        \"type\": L[\"type\"],\n",
    "        \"indices_meta\": {\n",
    "            \"stream\": \"indices\",\n",
    "            \"n_symbols\": int(indices.size),\n",
    "            \"code_lengths\": {str(int(k)): int(v) for k, v in len_idx.items()},\n",
    "            \"bit_length\": int(idx_bits),\n",
    "        },\n",
    "        \"col_delta_meta\": {\n",
    "            \"stream\": \"col_delta\",\n",
    "            \"n_symbols\": int(deltas.size),\n",
    "            \"code_lengths\": {str(int(k)): int(v) for k, v in len_col.items()},\n",
    "            \"bit_length\": int(col_bits),\n",
    "        },\n",
    "        \"rowptr_delta_meta\": meta_rpd,\n",
    "        \"files\": {\n",
    "            \"idx_bin\": f\"{name.replace('.', '_')}_idx.bin\",\n",
    "            \"col_bin\": f\"{name.replace('.', '_')}_col.bin\",\n",
    "            \"rpd_bin\": f\"{name.replace('.', '_')}_rpd.bin\" if COMPRESS_ROW_PTR else None,\n",
    "        }\n",
    "    }\n",
    "    jpath = HUF_DIR / f\"{name.replace('.', '_')}.json\"\n",
    "    jtxt = json.dumps(layer_meta, indent=2)\n",
    "    jpath.write_text(jtxt)\n",
    "    tot_json_bytes += jpath.stat().st_size\n",
    "\n",
    "    # 집계\n",
    "    tot_idx_bits  += idx_bits\n",
    "    tot_col_bits  += col_bits\n",
    "    tot_idx_bytes += len(idx_bytes)\n",
    "    tot_col_bytes += len(col_bytes)\n",
    "    if COMPRESS_ROW_PTR:\n",
    "        tot_rpd_bits  += rpd_bits\n",
    "        tot_rpd_bytes += len(rpd_bytes)\n",
    "\n",
    "    summary.append(layer_meta)\n",
    "\n",
    "# 전체 manifest\n",
    "manifest_huff = {\n",
    "    \"source_ws_dir\": str(WS_DIR),\n",
    "    \"compress_row_ptr\": COMPRESS_ROW_PTR,\n",
    "    \"layers\": summary,\n",
    "    \"totals\": {\n",
    "        \"idx_bits\": int(tot_idx_bits),\n",
    "        \"col_bits\": int(tot_col_bits),\n",
    "        \"rpd_bits\": int(tot_rpd_bits) if COMPRESS_ROW_PTR else 0,\n",
    "        \"idx_bytes\": int(tot_idx_bytes),\n",
    "        \"col_bytes\": int(tot_col_bytes),\n",
    "        \"rpd_bytes\": int(tot_rpd_bytes) if COMPRESS_ROW_PTR else 0,\n",
    "        \"json_bytes\": int(tot_json_bytes)\n",
    "    },\n",
    "    \"note\": \"bit_length은 패딩 제외 이론치, *_bytes는 패딩 포함 실제 파일 크기\"\n",
    "}\n",
    "(HUF_DIR / \"manifest_huff_ratio[0.9].json\").write_text(json.dumps(manifest_huff, indent=2))\n",
    "print(\"✅ Huffman encoding done:\", HUF_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "174e7318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Decode check: ALL OK\n"
     ]
    }
   ],
   "source": [
    "# H5) 허프만 디코딩 검증 (indices/col_delta/rowptr_delta)\n",
    "import json, numpy as np, random\n",
    "\n",
    "class BitReader:\n",
    "    def __init__(self, data: bytes, bit_length: int):\n",
    "        self.data = data; self.bit_length = bit_length; self.pos = 0\n",
    "    def read_bit(self):\n",
    "        if self.pos >= self.bit_length: raise EOFError\n",
    "        b = self.data[self.pos // 8]\n",
    "        bit = (b >> (7 - (self.pos % 8))) & 1\n",
    "        self.pos += 1\n",
    "        return bit\n",
    "\n",
    "def decode_canonical(br, lengths: dict, n_symbols: int) -> np.ndarray:\n",
    "    if not lengths or n_symbols == 0:\n",
    "        return np.zeros((0,), dtype=np.int64)\n",
    "    # 길이별 심볼 리스트 & 첫 코드\n",
    "    by_len = {}\n",
    "    for s, l in lengths.items():\n",
    "        by_len.setdefault(l, []).append(s)\n",
    "    for l in by_len: by_len[l].sort()\n",
    "    first_code, code, prev_len = {}, 0, None\n",
    "    for l in sorted(by_len):\n",
    "        if prev_len is not None: code <<= (l - prev_len)\n",
    "        first_code[l] = code\n",
    "        code += len(by_len[l]); prev_len = l\n",
    "    # 디코드\n",
    "    out = np.empty((n_symbols,), dtype=np.int64)\n",
    "    for i in range(n_symbols):\n",
    "        val = 0; l = 0\n",
    "        while True:\n",
    "            val = (val << 1) | br.read_bit(); l += 1\n",
    "            if l in first_code:\n",
    "                off = val - first_code[l]\n",
    "                if 0 <= off < len(by_len[l]):\n",
    "                    out[i] = by_len[l][off]; break\n",
    "    return out\n",
    "\n",
    "def rebuild_col_from_deltas(row_ptr: np.ndarray, deltas: np.ndarray) -> np.ndarray:\n",
    "    col = np.empty_like(deltas)\n",
    "    w = 0\n",
    "    for r in range(len(row_ptr)-1):\n",
    "        s, e = int(row_ptr[r]), int(row_ptr[r+1])\n",
    "        prev = 0\n",
    "        for _ in range(s, e):\n",
    "            d = int(deltas[w]); cur = prev + d\n",
    "            col[w] = cur; prev = cur; w += 1\n",
    "    return col\n",
    "\n",
    "ok = True\n",
    "for L in random.sample(mani[\"layers\"], k=min(5, len(mani[\"layers\"]))):\n",
    "    name = L[\"name\"]\n",
    "    csr = np.load(CSR_DIR / f\"{name.replace('.', '_')}.npz\")\n",
    "    row_ptr = csr[\"row_ptr\"].astype(np.int64)\n",
    "    col_ind = csr[\"col_ind\"].astype(np.int64)\n",
    "\n",
    "    meta = json.loads((HUF_DIR / f\"{name.replace('.', '_')}.json\").read_text())\n",
    "\n",
    "    # indices\n",
    "    idx_meta = meta[\"indices_meta\"]\n",
    "    idx_data = (HUF_DIR / f\"{name.replace('.', '_')}_idx.bin\").read_bytes()\n",
    "    br = BitReader(idx_data, int(idx_meta[\"bit_length\"]))\n",
    "    idx_lengths = {int(k): int(v) for k, v in idx_meta[\"code_lengths\"].items()}\n",
    "    dec_idx = decode_canonical(br, idx_lengths, int(idx_meta[\"n_symbols\"]))\n",
    "    ws = np.load(WS_DIR / f\"{name.replace('.', '_')}.npz\")\n",
    "    ok &= np.array_equal(dec_idx, ws[\"indices\"].astype(np.int64))\n",
    "\n",
    "    # col_delta\n",
    "    col_meta = meta[\"col_delta_meta\"]\n",
    "    col_data = (HUF_DIR / f\"{name.replace('.', '_')}_col.bin\").read_bytes()\n",
    "    br = BitReader(col_data, int(col_meta[\"bit_length\"]))\n",
    "    col_lengths = {int(k): int(v) for k, v in col_meta[\"code_lengths\"].items()}\n",
    "    dec_col_delta = decode_canonical(br, col_lengths, int(col_meta[\"n_symbols\"]))\n",
    "    ok &= np.array_equal(dec_col_delta, col_ind_to_deltas_per_row(row_ptr, col_ind))\n",
    "    ok &= np.array_equal(rebuild_col_from_deltas(row_ptr, dec_col_delta), col_ind)\n",
    "\n",
    "print(\"✅ Decode check:\", \"ALL OK\" if ok else \"MISMATCH FOUND\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "606ec3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== TOTAL ==\n",
      "orignal_MB: 28.862 MB\n",
      "purning_MB: 5.855 MB\n",
      "ws_MB: 0.725 MB\n",
      "huf_MB (final model size): 0.957 MB\n",
      "FINAL Huffman compression ratio: x30.14\n"
     ]
    }
   ],
   "source": [
    "# H6) 압축률 리포트 (Dense / CSR / WS / WS+Huffman[bin+json 합])\n",
    "import numpy as np, json, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def size_dense_bytes(shape2d):  # FP32 기준\n",
    "    r, c = shape2d\n",
    "    return int(r * c * 4)\n",
    "\n",
    "def size_csr_bytes(layer):\n",
    "    z = np.load(CSR_DIR / f\"{layer.replace('.', '_')}.npz\")\n",
    "    return int(z[\"row_ptr\"].nbytes + z[\"col_ind\"].nbytes + z[\"values\"].nbytes)\n",
    "\n",
    "def size_ws_bytes(layer):\n",
    "    z = np.load(WS_DIR / f\"{layer.replace('.', '_')}.npz\")\n",
    "    return int(z[\"codebook\"].nbytes + z[\"indices\"].nbytes)\n",
    "\n",
    "def size_ws_huf_bin_bytes(layer):\n",
    "    # .bin 합 (json 제외)\n",
    "    j = json.loads((HUF_DIR / f\"{layer.replace('.', '_')}.json\").read_text())\n",
    "    s = 0\n",
    "    s += (HUF_DIR / j[\"files\"][\"idx_bin\"]).stat().st_size\n",
    "    s += (HUF_DIR / j[\"files\"][\"col_bin\"]).stat().st_size\n",
    "    if j[\"files\"][\"rpd_bin\"]:\n",
    "        s += (HUF_DIR / j[\"files\"][\"rpd_bin\"]).stat().st_size\n",
    "    return s\n",
    "\n",
    "def size_ws_huf_json_bytes(layer):\n",
    "    return (HUF_DIR / f\"{layer.replace('.', '_')}.json\").stat().st_size\n",
    "\n",
    "rows = []\n",
    "for L in mani[\"layers\"]:\n",
    "    name = L[\"name\"]\n",
    "    dense_b = size_dense_bytes(tuple(L[\"shape2d\"]))\n",
    "    csr_b   = size_csr_bytes(name)\n",
    "    ws_b    = size_ws_bytes(name)\n",
    "    huf_bin_b  = size_ws_huf_bin_bytes(name)\n",
    "    huf_json_b = size_ws_huf_json_bytes(name)\n",
    "    rows.append({\n",
    "        \"layer\": name,\n",
    "        \"dense_bytes\": dense_b,\n",
    "        \"csr_bytes\": csr_b,\n",
    "        \"ws_bytes\": ws_b,\n",
    "        \"ws_huf_bin_json_bytes\": huf_bin_b + huf_json_b,  # ★ bin+json 합만 사용\n",
    "        \"x_dense_to_csr\": dense_b / max(1, csr_b),\n",
    "        \"x_dense_to_ws\": dense_b / max(1, ws_b),\n",
    "        \"x_dense_to_ws_huf_bin_json\": dense_b / max(1, (huf_bin_b + huf_json_b))  # ★ 최종 배율\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"compress_report_per_layer_ratio[0.9].csv\", index=False)\n",
    "\n",
    "# Totals\n",
    "dense_sum = int(df[\"dense_bytes\"].sum())\n",
    "csr_sum   = int(df[\"csr_bytes\"].sum())\n",
    "ws_sum    = int(df[\"ws_bytes\"].sum())\n",
    "huf_sum   = int(df[\"ws_huf_bin_json_bytes\"].sum())\n",
    "\n",
    "totals = {\n",
    "    \"dense_MB\": dense_sum / 1e6,\n",
    "    \"csr_MB\": csr_sum / 1e6,\n",
    "    \"ws_MB\": ws_sum / 1e6,\n",
    "    \"ws_huf_bin_json_MB\": huf_sum / 1e6,  # ★ 최종 모델 크기(bin+json)\n",
    "    \"x_dense_to_csr\": dense_sum / max(1, csr_sum),\n",
    "    \"x_dense_to_ws\": dense_sum / max(1, ws_sum),\n",
    "    \"x_dense_to_ws_huf_bin_json\": dense_sum / max(1, huf_sum)  # ★ 최종 압축배율\n",
    "}\n",
    "json.dump(totals, open(\"compress_report_total_ratio[0.9].json\", \"w\"), indent=2)\n",
    "pd.DataFrame([totals]).to_csv(\"compress_report_total_ratio[0.9].csv\", index=False)\n",
    "\n",
    "# 원하는 형태로만 출력\n",
    "print(\"== TOTAL ==\")\n",
    "print(f\"orignal_MB: {totals['dense_MB']:.3f} MB\")\n",
    "print(f\"purning_MB: {totals['csr_MB']:.3f} MB\")\n",
    "print(f\"ws_MB: {totals['ws_MB']:.3f} MB\")\n",
    "print(f\"huf_MB (final model size): {totals['ws_huf_bin_json_MB']:.3f} MB\")\n",
    "print(f\"FINAL Huffman compression ratio: x{totals['x_dense_to_ws_huf_bin_json']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1a657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ saved: huffman_results_bundle.zip\n"
     ]
    }
   ],
   "source": [
    "# H7) 결과물 ZIP (huff_dump/* + 리포트들)\n",
    "import zipfile, os\n",
    "with zipfile.ZipFile(\"huffman_results_bundle.zip_ratio[0.9]\", \"w\", zipfile.ZIP_DEFLATED) as z:\n",
    "    # encodings\n",
    "    for p in HUF_DIR.glob(\"*\"):\n",
    "        z.write(p, arcname=f\"huff_dump/{p.name}\")\n",
    "    # reports\n",
    "    for name in [\"compress_report_per_layer.csv\", \"compress_report_total.csv\", \"compress_report_total.json\"]:\n",
    "        if Path(name).exists():\n",
    "            z.write(name, arcname=name)\n",
    "print(\"✅ saved: huffman_results_bundle.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdff1a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/yyb02274/yolov5\n",
      "Loaded model: <class 'models.yolo.DetectionModel'>\n"
     ]
    }
   ],
   "source": [
    "# === 경로 설정 ===\n",
    "PRUNED_MODEL_PATH = \"/home/yyb02274/yolov5/runs/train/retrain_pruned/weights/best.pt\"\n",
    "DATA_YAML         = \"data/coco128.yaml\"\n",
    "ABS_TRAIN = \"/home/yyb02274/datasets/coco128/images/train2017\"\n",
    "ABS_VAL   = \"/home/yyb02274/datasets/coco128/images/train2017\"\n",
    "\n",
    "CSR_DIR = \"csr_dump_ratio[0.9]\"\n",
    "WS_DIR  = \"ws_dump_ratio[0.9]\"\n",
    "HUF_DIR = \"huff_dump_ratio[0.9]\"\n",
    "\n",
    "PRUNE_TAG = \"ratio[0.9]\"  # ← 파일명 접미사로 사용\n",
    "\n",
    "# YOLOv5 레포 루트로 이동(또는 sys.path 추가)\n",
    "import os, sys, torch\n",
    "os.chdir(\"/home/yyb02274/yolov5\")  # ← 너의 yolov5 루트\n",
    "print(\"CWD:\", os.getcwd())\n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dm = DetectMultiBackend(PRUNED_MODEL_PATH, device=device, dnn=False, fuse=False)\n",
    "model = dm.model.to(device).eval()\n",
    "print(\"Loaded model:\", type(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fca46a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injected Huffman-decoded WS weights. mismatches: 0 []\n"
     ]
    }
   ],
   "source": [
    "import json, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "CSR_DIR = Path(CSR_DIR); WS_DIR = Path(WS_DIR); HUF_DIR = Path(HUF_DIR)\n",
    "mani = json.loads((CSR_DIR / \"manifest_ratio[0.9].json\").read_text())\n",
    "\n",
    "# ----- Huffman BitReader & canonical decode -----\n",
    "class BitReader:\n",
    "    def __init__(self, data: bytes, bit_length: int):\n",
    "        self.data = data; self.bit_length = bit_length; self.pos = 0\n",
    "    def read_bit(self):\n",
    "        if self.pos >= self.bit_length: raise EOFError\n",
    "        b = self.data[self.pos // 8]\n",
    "        bit = (b >> (7 - (self.pos % 8))) & 1\n",
    "        self.pos += 1\n",
    "        return bit\n",
    "\n",
    "def decode_canonical(br, lengths: dict[int,int], n_symbols: int) -> np.ndarray:\n",
    "    if not lengths or n_symbols == 0:\n",
    "        return np.zeros((0,), dtype=np.int64)\n",
    "    by_len = {}\n",
    "    for s, l in lengths.items():\n",
    "        by_len.setdefault(int(l), []).append(int(s))\n",
    "    for l in by_len: by_len[l].sort()\n",
    "    first_code, code, prev_len = {}, 0, None\n",
    "    for l in sorted(by_len):\n",
    "        if prev_len is not None: code <<= (l - prev_len)\n",
    "        first_code[l] = code\n",
    "        code += len(by_len[l]); prev_len = l\n",
    "    out = np.empty((n_symbols,), dtype=np.int64)\n",
    "    for i in range(n_symbols):\n",
    "        val = 0; l = 0\n",
    "        while True:\n",
    "            val = (val << 1) | br.read_bit(); l += 1\n",
    "            if l in first_code:\n",
    "                off = val - first_code[l]\n",
    "                if 0 <= off < len(by_len[l]):\n",
    "                    out[i] = by_len[l][off]; break\n",
    "    return out\n",
    "\n",
    "def rebuild_col_from_deltas(row_ptr: np.ndarray, deltas: np.ndarray) -> np.ndarray:\n",
    "    col = np.empty_like(deltas)\n",
    "    w = 0\n",
    "    for r in range(len(row_ptr)-1):\n",
    "        s, e = int(row_ptr[r]), int(row_ptr[r+1])\n",
    "        prev = 0\n",
    "        for _ in range(s, e):\n",
    "            d = int(deltas[w]); cur = prev + d\n",
    "            col[w] = cur; prev = cur; w += 1\n",
    "    return col\n",
    "\n",
    "def csr_to_dense(row_ptr, col_ind, values, shape2d):\n",
    "    rows, cols = shape2d\n",
    "    dense = np.zeros((rows, cols), dtype=np.float32)\n",
    "    for r in range(rows):\n",
    "        s, e = int(row_ptr[r]), int(row_ptr[r+1])\n",
    "        if e > s:\n",
    "            dense[r, col_ind[s:e]] = values[s:e]\n",
    "    return dense\n",
    "\n",
    "# ----- 레이어별: .json 메타 + .bin 읽고 허프만 디코딩 → WS codebook으로 values 복원 → dense → 주입 -----\n",
    "module_map = dict(model.named_modules())\n",
    "mismatch = []\n",
    "\n",
    "for L in mani[\"layers\"]:\n",
    "    name = L[\"name\"]; rows, cols = L[\"shape2d\"]; is_conv = (L[\"type\"] == \"conv2d\")\n",
    "\n",
    "    # per-layer meta\n",
    "    meta = json.loads((HUF_DIR / f\"{name.replace('.','_')}.json\").read_text())\n",
    "\n",
    "    # row_ptr: rpd 압축 했으면 복원, 아니면 CSR에서 로드\n",
    "    if meta.get(\"rowptr_delta_meta\"):\n",
    "        m = meta[\"rowptr_delta_meta\"]\n",
    "        data = (HUF_DIR / f\"{name.replace('.','_')}_rpd.bin\").read_bytes()\n",
    "        br = BitReader(data, int(m[\"bit_length\"]))\n",
    "        lengths = {int(k): int(v) for k, v in m[\"code_lengths\"].items()}\n",
    "        rpd = decode_canonical(br, lengths, int(m[\"n_symbols\"]))\n",
    "        row_ptr = np.zeros((len(rpd)+1,), dtype=np.int64); row_ptr[1:] = np.cumsum(rpd, dtype=np.int64)\n",
    "    else:\n",
    "        row_ptr = np.load(CSR_DIR / f\"{name.replace('.','_')}.npz\")[\"row_ptr\"].astype(np.int64)\n",
    "\n",
    "    # col_delta → col_ind\n",
    "    m = meta[\"col_delta_meta\"]\n",
    "    data = (HUF_DIR / f\"{name.replace('.','_')}_col.bin\").read_bytes()\n",
    "    br = BitReader(data, int(m[\"bit_length\"]))\n",
    "    lengths = {int(k): int(v) for k, v in m[\"code_lengths\"].items()}\n",
    "    deltas = decode_canonical(br, lengths, int(m[\"n_symbols\"]))\n",
    "    col_ind = rebuild_col_from_deltas(row_ptr, deltas)\n",
    "\n",
    "    # indices → values via WS codebook\n",
    "    m = meta[\"indices_meta\"]\n",
    "    data = (HUF_DIR / f\"{name.replace('.','_')}_idx.bin\").read_bytes()\n",
    "    br = BitReader(data, int(m[\"bit_length\"]))\n",
    "    lengths = {int(k): int(v) for k, v in m[\"code_lengths\"].items()}\n",
    "    indices = decode_canonical(br, lengths, int(m[\"n_symbols\"]))\n",
    "    ws = np.load(WS_DIR / f\"{name.replace('.','_')}.npz\")\n",
    "    codebook = ws[\"codebook\"].astype(np.float32)\n",
    "    values = codebook[indices] if indices.size else np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "    dense2d = csr_to_dense(row_ptr, col_ind, values, (rows, cols))\n",
    "    if is_conv:\n",
    "        O, I, kH, kW = L[\"shape\"]; Wrec = dense2d.reshape(O, I, kH, kW)\n",
    "    else:\n",
    "        Wrec = dense2d\n",
    "\n",
    "    mod = module_map.get(name, None)\n",
    "    if (mod is None) or (not hasattr(mod, \"weight\")):\n",
    "        mismatch.append(name); continue\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        mod.weight.data = torch.from_numpy(Wrec).to(mod.weight.data.device, dtype=mod.weight.data.dtype)\n",
    "\n",
    "print(\"Injected Huffman-decoded WS weights. mismatches:\", len(mismatch), mismatch[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36416e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: Scanning /home/yyb02274/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val images: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 8/8 [00:01<00:00,  7.16it/s]\n",
      "                   all        128        929     0.0188     0.0217     0.0127    0.00658\n",
      "                person        128        254     0.0114      0.236     0.0187    0.00504\n",
      "               bicycle        128          6          0          0          0          0\n",
      "                   car        128         46          0          0          0          0\n",
      "            motorcycle        128          5          0          0          0          0\n",
      "              airplane        128          6     0.0294      0.333     0.0402     0.0203\n",
      "                   bus        128          7          0          0          0          0\n",
      "                 train        128          3          0          0          0          0\n",
      "                 truck        128         12          0          0          0          0\n",
      "                  boat        128          6          0          0          0          0\n",
      "         traffic light        128         14          0          0          0          0\n",
      "             stop sign        128          2          0          0          0          0\n",
      "                 bench        128          9          0          0          0          0\n",
      "                  bird        128         16          0          0          0          0\n",
      "                   cat        128          4     0.0556       0.25     0.0386    0.00386\n",
      "                   dog        128          9          0          0          0          0\n",
      "                 horse        128          2          0          0          0          0\n",
      "              elephant        128         17          0          0          0          0\n",
      "                  bear        128          1          0          0          0          0\n",
      "                 zebra        128          4          0          0          0          0\n",
      "               giraffe        128          9          0          0          0          0\n",
      "              backpack        128          6          0          0          0          0\n",
      "              umbrella        128         18          0          0          0          0\n",
      "               handbag        128         19          0          0          0          0\n",
      "                   tie        128          7          0          0          0          0\n",
      "              suitcase        128          4          0          0          0          0\n",
      "               frisbee        128          5          0          0          0          0\n",
      "                  skis        128          1          0          0          0          0\n",
      "             snowboard        128          7          0          0          0          0\n",
      "           sports ball        128          6          0          0          0          0\n",
      "                  kite        128         10          0          0          0          0\n",
      "          baseball bat        128          4          0          0          0          0\n",
      "        baseball glove        128          7          0          0          0          0\n",
      "            skateboard        128          5          0          0          0          0\n",
      "         tennis racket        128          7          0          0          0          0\n",
      "                bottle        128         18          0          0          0          0\n",
      "            wine glass        128         16          0          0          0          0\n",
      "                   cup        128         36          0          0          0          0\n",
      "                  fork        128          6          0          0          0          0\n",
      "                 knife        128         16          0          0          0          0\n",
      "                 spoon        128         22          0          0          0          0\n",
      "                  bowl        128         28          0          0          0          0\n",
      "                banana        128          1          0          0          0          0\n",
      "              sandwich        128          2          0          0          0          0\n",
      "                orange        128          4          0          0          0          0\n",
      "              broccoli        128         11          0          0          0          0\n",
      "                carrot        128         24     0.0488     0.0833     0.0345     0.0103\n",
      "               hot dog        128          2          0          0          0          0\n",
      "                 pizza        128          5          0          0          0          0\n",
      "                 donut        128         14          1     0.0714      0.536      0.321\n",
      "                  cake        128          4          0          0          0          0\n",
      "                 chair        128         35          0          0          0          0\n",
      "                 couch        128          6          0          0          0          0\n",
      "          potted plant        128         14          0          0          0          0\n",
      "                   bed        128          3    0.00417      0.333    0.00624    0.00125\n",
      "          dining table        128         13     0.0172     0.0769     0.0124    0.00372\n",
      "                toilet        128          2          0          0          0          0\n",
      "                    tv        128          2          0          0          0          0\n",
      "                laptop        128          3          0          0          0          0\n",
      "                 mouse        128          2          0          0          0          0\n",
      "                remote        128          8          0          0          0          0\n",
      "            cell phone        128          8          0          0          0          0\n",
      "             microwave        128          3          0          0          0          0\n",
      "                  oven        128          5          0          0          0          0\n",
      "                  sink        128          6          0          0          0          0\n",
      "          refrigerator        128          5          0          0          0          0\n",
      "                  book        128         29          0          0          0          0\n",
      "                 clock        128          9      0.111      0.111      0.164     0.0656\n",
      "                  vase        128          2          0          0          0          0\n",
      "              scissors        128          1          0          0          0          0\n",
      "            teddy bear        128         21     0.0588     0.0476     0.0507     0.0355\n",
      "            toothbrush        128          5          0          0          0          0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUF_ratio[0.9]] P=0.0188 R=0.0217 mAP@0.5=0.0127 mAP@0.5:.95=0.0066\n",
      "[HUF_ratio[0.9]] val/box=0.00000 val/obj=0.00000 val/cls=0.00000\n",
      "Saved: huff_eval_metrics_ratio[0.9].csv\n"
     ]
    }
   ],
   "source": [
    "from utils.general import check_yaml\n",
    "try:\n",
    "    from utils.dataloaders import check_dataset, create_dataloader\n",
    "except:\n",
    "    from utils.general import check_dataset\n",
    "    from utils.dataloaders import create_dataloader\n",
    "\n",
    "# yaml->dict + 경로 정규화\n",
    "data_dict = check_dataset(check_yaml(DATA_YAML))\n",
    "\n",
    "def norm_path(p):\n",
    "    s = str(p).replace(\"／\", \"/\").replace(\"\\\\\", \"/\")\n",
    "    while '//' in s:\n",
    "        s = s.replace('//', '/')\n",
    "    return s\n",
    "\n",
    "for k in (\"train\",\"val\",\"test\"):\n",
    "    if k in data_dict and data_dict[k]:\n",
    "        data_dict[k] = norm_path(data_dict[k])\n",
    "\n",
    "# 필요 시 절대경로 덮어쓰기(네 환경에 맞게)\n",
    "data_dict[\"train\"] = ABS_TRAIN\n",
    "data_dict[\"val\"]   = ABS_VAL\n",
    "\n",
    "import torch\n",
    "stride = int(getattr(model, 'stride', torch.tensor([32])).max())\n",
    "IMGSZ, BATCH, WORKERS = 640, 16, 4\n",
    "\n",
    "dataloader = create_dataloader(\n",
    "    data_dict[\"val\"], IMGSZ, BATCH, stride,\n",
    "    single_cls=False, pad=0.5, rect=True, workers=WORKERS, prefix=\"val: \"\n",
    ")[0]\n",
    "print(\"val images:\", len(dataloader.dataset))\n",
    "\n",
    "# 평가\n",
    "from val import run as val_run\n",
    "import pandas as pd\n",
    "\n",
    "DEVICE = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "results, maps, _ = val_run(\n",
    "    data=data_dict,\n",
    "    dataloader=dataloader,\n",
    "    imgsz=IMGSZ,\n",
    "    batch_size=BATCH,\n",
    "    model=model,\n",
    "    device=DEVICE,\n",
    "    iou_thres=0.6,\n",
    "    single_cls=False,\n",
    "    verbose=True,\n",
    "    plots=False\n",
    ")\n",
    "\n",
    "P, R, mAP50, mAP5095, vbox, vobj, vcls = results\n",
    "print(f\"[HUF_{PRUNE_TAG}] P={P:.4f} R={R:.4f} mAP@0.5={mAP50:.4f} mAP@0.5:.95={mAP5095:.4f}\")\n",
    "print(f\"[HUF_{PRUNE_TAG}] val/box={vbox:.5f} val/obj={vobj:.5f} val/cls={vcls:.5f}\")\n",
    "\n",
    "row = {\n",
    "    \"stage\": f\"pruned+WS+HUF_{PRUNE_TAG}\",\n",
    "    \"metrics/precision\": P,\n",
    "    \"metrics/recall\": R,\n",
    "    \"metrics/mAP_0.5\": mAP50,\n",
    "    \"metrics/mAP_0.5:0.95\": mAP5095,\n",
    "    \"val/box_loss\": vbox,\n",
    "    \"val/obj_loss\": vobj,\n",
    "    \"val/cls_loss\": vcls,\n",
    "    \"imgsz\": IMGSZ, \"batch\": BATCH\n",
    "}\n",
    "save_name = f\"huff_eval_metrics_{PRUNE_TAG}.csv\"\n",
    "pd.DataFrame([row]).to_csv(save_name, index=False)\n",
    "print(\"Saved:\", save_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
