{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1b6edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/yyb02274/yolov5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/yyb02274/yolov5\")  # ÎÑ§ yolov5 Î£®Ìä∏ Í≤ΩÎ°ú\n",
    "print(\"CWD:\", os.getcwd())\n",
    "\n",
    "from models.yolo import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b3dc2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Î™®Îç∏ Íµ¨Ï°∞ Î∞è Í∞ÄÏ§ëÏπò Î°úÎî© ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "from models.yolo import Model\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "# ÏÑ§Ï†ï\n",
    "cfg_path = 'models/yolov5s.yaml'\n",
    "weights_path = 'runs/train/baseline_yolov5s/weights/best.pt'\n",
    "num_classes = 80\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Î™®Îç∏ Íµ¨Ï°∞ ÏÉùÏÑ±\n",
    "with open(cfg_path) as f:\n",
    "    model_cfg = yaml.safe_load(f)\n",
    "\n",
    "model = Model(model_cfg, ch=3, nc=num_classes).to(device)\n",
    "model.eval()\n",
    "\n",
    "checkpoint = torch.load(weights_path, map_location=device, weights_only=False)\n",
    "state_dict = checkpoint['model'].state_dict()\n",
    "model.load_state_dict(state_dict)\n",
    "print(\"‚úÖ Î™®Îç∏ Íµ¨Ï°∞ Î∞è Í∞ÄÏ§ëÏπò Î°úÎî© ÏôÑÎ£å\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7a322c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Unstructured Pruning ÏãúÏûë (ÎπÑÏú®: 90%)\n",
      " ‚Üí model.0.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.2.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.2.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.2.cv3.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.2.m.0.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.2.m.0.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.3.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.4.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.4.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.4.cv3.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.4.m.0.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.4.m.0.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.4.m.1.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.4.m.1.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.5.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.6.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.6.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.6.cv3.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.6.m.0.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.6.m.0.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.6.m.1.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.6.m.1.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.6.m.2.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.6.m.2.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.7.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.8.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.8.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.8.cv3.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.8.m.0.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.8.m.0.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.9.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.9.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.10.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.13.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.13.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.13.cv3.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.13.m.0.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.13.m.0.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.14.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.17.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.17.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.17.cv3.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.17.m.0.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.17.m.0.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.18.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.20.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.20.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.20.cv3.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.20.m.0.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.20.m.0.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.21.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.23.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.23.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.23.cv3.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.23.m.0.cv1.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.23.m.0.cv2.conv pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.24.m.0 pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.24.m.1 pruning Ï†ÅÏö© Ï§ë...\n",
      " ‚Üí model.24.m.2 pruning Ï†ÅÏö© Ï§ë...\n",
      "Í∞ÄÏßÄÏπòÍ∏∞ ÏûÑÏãú Ï†ÅÏö© ÏôÑÎ£å (reparametrization ÏÉÅÌÉú)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn as nn\n",
    "\n",
    "prune_rate = 0.9\n",
    "modules_to_prune = []\n",
    "\n",
    "print(f\"L1 Unstructured Pruning ÏãúÏûë (ÎπÑÏú®: {prune_rate * 100:.0f}%)\")\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        print(f\" ‚Üí {name} pruning Ï†ÅÏö© Ï§ë...\")\n",
    "        prune.l1_unstructured(module, name='weight', amount=prune_rate)\n",
    "        modules_to_prune.append((module, 'weight'))\n",
    "\n",
    "print(\"Í∞ÄÏßÄÏπòÍ∏∞ ÏûÑÏãú Ï†ÅÏö© ÏôÑÎ£å (reparametrization ÏÉÅÌÉú)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc29fd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ pruning Í≤∞Í≥ºÎ•º Î™®Îç∏Ïóê ÏòÅÍµ¨ Î∞òÏòÅÌñàÏäµÎãàÎã§ (weight_mask Ï†úÍ±∞ ÏôÑÎ£å)\n",
      "weight_orig Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏ (ÏÉÅÏúÑ 3Í∞ú):\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for module, param_name in modules_to_prune:\n",
    "    \n",
    "    if hasattr(module, param_name + \"_mask\"):\n",
    "        prune.remove(module, param_name)\n",
    "\n",
    "print(\"‚úÖ pruning Í≤∞Í≥ºÎ•º Î™®Îç∏Ïóê ÏòÅÍµ¨ Î∞òÏòÅÌñàÏäµÎãàÎã§ (weight_mask Ï†úÍ±∞ ÏôÑÎ£å)\")\n",
    "\n",
    "print(\"weight_orig Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏ (ÏÉÅÏúÑ 3Í∞ú):\")\n",
    "for module, _ in modules_to_prune[:3]:\n",
    "    print('weight_orig' in module._parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a79d77ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ YOLOv5Ïö© checkpoint Ï†ÄÏû• ÏôÑÎ£å: runs/train/baseline_yolov5s/weights/pruned_ratio[0.9].pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ckpt_path = \"runs/train/baseline_yolov5s/weights/pruned_ratio[0.9].pt\"\n",
    "torch.save({'model': model}, ckpt_path)\n",
    "print(f\"üì¶ YOLOv5Ïö© checkpoint Ï†ÄÏû• ÏôÑÎ£å: {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1afaf294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Ï†ÑÏ≤¥ sparsity: 89.88%\n",
      "   ‚Üí 0Ïù∏ weight Ïàò: 6,494,054 / Ï†ÑÏ≤¥ weight Ïàò: 7,225,120\n"
     ]
    }
   ],
   "source": [
    "def calculate_global_sparsity(model):\n",
    "    total_zeros = 0\n",
    "    total_elements = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name and param.requires_grad:\n",
    "            total_zeros += torch.sum(param == 0).item()\n",
    "            total_elements += param.numel()\n",
    "    sparsity = 100.0 * total_zeros / total_elements\n",
    "    return sparsity, total_zeros, total_elements\n",
    "\n",
    "sparsity, total_zeros, total_elements = calculate_global_sparsity(model)\n",
    "print(f\"üìâ Ï†ÑÏ≤¥ sparsity: {sparsity:.2f}%\")\n",
    "print(f\"   ‚Üí 0Ïù∏ weight Ïàò: {total_zeros:,} / Ï†ÑÏ≤¥ weight Ïàò: {total_elements:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf3a8637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv.weight                      | nonzeros =       346 /      3456 ( 10.01%) | total_pruned =      3110 | shape = (32, 3, 6, 6)\n",
      "model.0.bn.weight                        | nonzeros =        32 /        32 (100.00%) | total_pruned =         0 | shape = (32,)\n",
      "model.0.bn.bias                          | nonzeros =        32 /        32 (100.00%) | total_pruned =         0 | shape = (32,)\n",
      "model.1.conv.weight                      | nonzeros =      1843 /     18432 ( 10.00%) | total_pruned =     16589 | shape = (64, 32, 3, 3)\n",
      "model.1.bn.weight                        | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.1.bn.bias                          | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.2.cv1.conv.weight                  | nonzeros =       205 /      2048 ( 10.01%) | total_pruned =      1843 | shape = (32, 64, 1, 1)\n",
      "model.2.cv1.bn.weight                    | nonzeros =        32 /        32 (100.00%) | total_pruned =         0 | shape = (32,)\n",
      "model.2.cv1.bn.bias                      | nonzeros =        32 /        32 (100.00%) | total_pruned =         0 | shape = (32,)\n",
      "model.2.cv2.conv.weight                  | nonzeros =       205 /      2048 ( 10.01%) | total_pruned =      1843 | shape = (32, 64, 1, 1)\n",
      "model.2.cv2.bn.weight                    | nonzeros =        32 /        32 (100.00%) | total_pruned =         0 | shape = (32,)\n",
      "model.2.cv2.bn.bias                      | nonzeros =        32 /        32 (100.00%) | total_pruned =         0 | shape = (32,)\n",
      "model.2.cv3.conv.weight                  | nonzeros =       410 /      4096 ( 10.01%) | total_pruned =      3686 | shape = (64, 64, 1, 1)\n",
      "model.2.cv3.bn.weight                    | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.2.cv3.bn.bias                      | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.2.m.0.cv1.conv.weight              | nonzeros =       102 /      1024 (  9.96%) | total_pruned =       922 | shape = (32, 32, 1, 1)\n",
      "model.2.m.0.cv1.bn.weight                | nonzeros =        32 /        32 (100.00%) | total_pruned =         0 | shape = (32,)\n",
      "model.2.m.0.cv1.bn.bias                  | nonzeros =        32 /        32 (100.00%) | total_pruned =         0 | shape = (32,)\n",
      "model.2.m.0.cv2.conv.weight              | nonzeros =       922 /      9216 ( 10.00%) | total_pruned =      8294 | shape = (32, 32, 3, 3)\n",
      "model.2.m.0.cv2.bn.weight                | nonzeros =        32 /        32 (100.00%) | total_pruned =         0 | shape = (32,)\n",
      "model.2.m.0.cv2.bn.bias                  | nonzeros =        32 /        32 (100.00%) | total_pruned =         0 | shape = (32,)\n",
      "model.3.conv.weight                      | nonzeros =      7373 /     73728 ( 10.00%) | total_pruned =     66355 | shape = (128, 64, 3, 3)\n",
      "model.3.bn.weight                        | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.3.bn.bias                          | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.4.cv1.conv.weight                  | nonzeros =       819 /      8192 ( 10.00%) | total_pruned =      7373 | shape = (64, 128, 1, 1)\n",
      "model.4.cv1.bn.weight                    | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.4.cv1.bn.bias                      | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.4.cv2.conv.weight                  | nonzeros =       819 /      8192 ( 10.00%) | total_pruned =      7373 | shape = (64, 128, 1, 1)\n",
      "model.4.cv2.bn.weight                    | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.4.cv2.bn.bias                      | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.4.cv3.conv.weight                  | nonzeros =      1638 /     16384 ( 10.00%) | total_pruned =     14746 | shape = (128, 128, 1, 1)\n",
      "model.4.cv3.bn.weight                    | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.4.cv3.bn.bias                      | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.4.m.0.cv1.conv.weight              | nonzeros =       410 /      4096 ( 10.01%) | total_pruned =      3686 | shape = (64, 64, 1, 1)\n",
      "model.4.m.0.cv1.bn.weight                | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.4.m.0.cv1.bn.bias                  | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.4.m.0.cv2.conv.weight              | nonzeros =      3686 /     36864 ( 10.00%) | total_pruned =     33178 | shape = (64, 64, 3, 3)\n",
      "model.4.m.0.cv2.bn.weight                | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.4.m.0.cv2.bn.bias                  | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.4.m.1.cv1.conv.weight              | nonzeros =       410 /      4096 ( 10.01%) | total_pruned =      3686 | shape = (64, 64, 1, 1)\n",
      "model.4.m.1.cv1.bn.weight                | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.4.m.1.cv1.bn.bias                  | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.4.m.1.cv2.conv.weight              | nonzeros =      3686 /     36864 ( 10.00%) | total_pruned =     33178 | shape = (64, 64, 3, 3)\n",
      "model.4.m.1.cv2.bn.weight                | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.4.m.1.cv2.bn.bias                  | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.5.conv.weight                      | nonzeros =     29491 /    294912 ( 10.00%) | total_pruned =    265421 | shape = (256, 128, 3, 3)\n",
      "model.5.bn.weight                        | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.5.bn.bias                          | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.6.cv1.conv.weight                  | nonzeros =      3277 /     32768 ( 10.00%) | total_pruned =     29491 | shape = (128, 256, 1, 1)\n",
      "model.6.cv1.bn.weight                    | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.6.cv1.bn.bias                      | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.6.cv2.conv.weight                  | nonzeros =      3277 /     32768 ( 10.00%) | total_pruned =     29491 | shape = (128, 256, 1, 1)\n",
      "model.6.cv2.bn.weight                    | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.6.cv2.bn.bias                      | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.6.cv3.conv.weight                  | nonzeros =      6554 /     65536 ( 10.00%) | total_pruned =     58982 | shape = (256, 256, 1, 1)\n",
      "model.6.cv3.bn.weight                    | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.6.cv3.bn.bias                      | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.6.m.0.cv1.conv.weight              | nonzeros =      1638 /     16384 ( 10.00%) | total_pruned =     14746 | shape = (128, 128, 1, 1)\n",
      "model.6.m.0.cv1.bn.weight                | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.6.m.0.cv1.bn.bias                  | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.6.m.0.cv2.conv.weight              | nonzeros =     14746 /    147456 ( 10.00%) | total_pruned =    132710 | shape = (128, 128, 3, 3)\n",
      "model.6.m.0.cv2.bn.weight                | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.6.m.0.cv2.bn.bias                  | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.6.m.1.cv1.conv.weight              | nonzeros =      1638 /     16384 ( 10.00%) | total_pruned =     14746 | shape = (128, 128, 1, 1)\n",
      "model.6.m.1.cv1.bn.weight                | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.6.m.1.cv1.bn.bias                  | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.6.m.1.cv2.conv.weight              | nonzeros =     14746 /    147456 ( 10.00%) | total_pruned =    132710 | shape = (128, 128, 3, 3)\n",
      "model.6.m.1.cv2.bn.weight                | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.6.m.1.cv2.bn.bias                  | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.6.m.2.cv1.conv.weight              | nonzeros =      1638 /     16384 ( 10.00%) | total_pruned =     14746 | shape = (128, 128, 1, 1)\n",
      "model.6.m.2.cv1.bn.weight                | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.6.m.2.cv1.bn.bias                  | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.6.m.2.cv2.conv.weight              | nonzeros =     14746 /    147456 ( 10.00%) | total_pruned =    132710 | shape = (128, 128, 3, 3)\n",
      "model.6.m.2.cv2.bn.weight                | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.6.m.2.cv2.bn.bias                  | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.7.conv.weight                      | nonzeros =    117965 /   1179648 ( 10.00%) | total_pruned =   1061683 | shape = (512, 256, 3, 3)\n",
      "model.7.bn.weight                        | nonzeros =       512 /       512 (100.00%) | total_pruned =         0 | shape = (512,)\n",
      "model.7.bn.bias                          | nonzeros =       512 /       512 (100.00%) | total_pruned =         0 | shape = (512,)\n",
      "model.8.cv1.conv.weight                  | nonzeros =     13107 /    131072 ( 10.00%) | total_pruned =    117965 | shape = (256, 512, 1, 1)\n",
      "model.8.cv1.bn.weight                    | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.8.cv1.bn.bias                      | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.8.cv2.conv.weight                  | nonzeros =     13107 /    131072 ( 10.00%) | total_pruned =    117965 | shape = (256, 512, 1, 1)\n",
      "model.8.cv2.bn.weight                    | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.8.cv2.bn.bias                      | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.8.cv3.conv.weight                  | nonzeros =     26214 /    262144 ( 10.00%) | total_pruned =    235930 | shape = (512, 512, 1, 1)\n",
      "model.8.cv3.bn.weight                    | nonzeros =       512 /       512 (100.00%) | total_pruned =         0 | shape = (512,)\n",
      "model.8.cv3.bn.bias                      | nonzeros =       512 /       512 (100.00%) | total_pruned =         0 | shape = (512,)\n",
      "model.8.m.0.cv1.conv.weight              | nonzeros =      6554 /     65536 ( 10.00%) | total_pruned =     58982 | shape = (256, 256, 1, 1)\n",
      "model.8.m.0.cv1.bn.weight                | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.8.m.0.cv1.bn.bias                  | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.8.m.0.cv2.conv.weight              | nonzeros =     58982 /    589824 ( 10.00%) | total_pruned =    530842 | shape = (256, 256, 3, 3)\n",
      "model.8.m.0.cv2.bn.weight                | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.8.m.0.cv2.bn.bias                  | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.9.cv1.conv.weight                  | nonzeros =     13107 /    131072 ( 10.00%) | total_pruned =    117965 | shape = (256, 512, 1, 1)\n",
      "model.9.cv1.bn.weight                    | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.9.cv1.bn.bias                      | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.9.cv2.conv.weight                  | nonzeros =     52429 /    524288 ( 10.00%) | total_pruned =    471859 | shape = (512, 1024, 1, 1)\n",
      "model.9.cv2.bn.weight                    | nonzeros =       512 /       512 (100.00%) | total_pruned =         0 | shape = (512,)\n",
      "model.9.cv2.bn.bias                      | nonzeros =       512 /       512 (100.00%) | total_pruned =         0 | shape = (512,)\n",
      "model.10.conv.weight                     | nonzeros =     13107 /    131072 ( 10.00%) | total_pruned =    117965 | shape = (256, 512, 1, 1)\n",
      "model.10.bn.weight                       | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.10.bn.bias                         | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.13.cv1.conv.weight                 | nonzeros =      6554 /     65536 ( 10.00%) | total_pruned =     58982 | shape = (128, 512, 1, 1)\n",
      "model.13.cv1.bn.weight                   | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.13.cv1.bn.bias                     | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.13.cv2.conv.weight                 | nonzeros =      6554 /     65536 ( 10.00%) | total_pruned =     58982 | shape = (128, 512, 1, 1)\n",
      "model.13.cv2.bn.weight                   | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.13.cv2.bn.bias                     | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.13.cv3.conv.weight                 | nonzeros =      6554 /     65536 ( 10.00%) | total_pruned =     58982 | shape = (256, 256, 1, 1)\n",
      "model.13.cv3.bn.weight                   | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.13.cv3.bn.bias                     | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.13.m.0.cv1.conv.weight             | nonzeros =      1638 /     16384 ( 10.00%) | total_pruned =     14746 | shape = (128, 128, 1, 1)\n",
      "model.13.m.0.cv1.bn.weight               | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.13.m.0.cv1.bn.bias                 | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.13.m.0.cv2.conv.weight             | nonzeros =     14746 /    147456 ( 10.00%) | total_pruned =    132710 | shape = (128, 128, 3, 3)\n",
      "model.13.m.0.cv2.bn.weight               | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.13.m.0.cv2.bn.bias                 | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.14.conv.weight                     | nonzeros =      3277 /     32768 ( 10.00%) | total_pruned =     29491 | shape = (128, 256, 1, 1)\n",
      "model.14.bn.weight                       | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.14.bn.bias                         | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.17.cv1.conv.weight                 | nonzeros =      1638 /     16384 ( 10.00%) | total_pruned =     14746 | shape = (64, 256, 1, 1)\n",
      "model.17.cv1.bn.weight                   | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.17.cv1.bn.bias                     | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.17.cv2.conv.weight                 | nonzeros =      1638 /     16384 ( 10.00%) | total_pruned =     14746 | shape = (64, 256, 1, 1)\n",
      "model.17.cv2.bn.weight                   | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.17.cv2.bn.bias                     | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.17.cv3.conv.weight                 | nonzeros =      1638 /     16384 ( 10.00%) | total_pruned =     14746 | shape = (128, 128, 1, 1)\n",
      "model.17.cv3.bn.weight                   | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.17.cv3.bn.bias                     | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.17.m.0.cv1.conv.weight             | nonzeros =       410 /      4096 ( 10.01%) | total_pruned =      3686 | shape = (64, 64, 1, 1)\n",
      "model.17.m.0.cv1.bn.weight               | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.17.m.0.cv1.bn.bias                 | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.17.m.0.cv2.conv.weight             | nonzeros =      3686 /     36864 ( 10.00%) | total_pruned =     33178 | shape = (64, 64, 3, 3)\n",
      "model.17.m.0.cv2.bn.weight               | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.17.m.0.cv2.bn.bias                 | nonzeros =        64 /        64 (100.00%) | total_pruned =         0 | shape = (64,)\n",
      "model.18.conv.weight                     | nonzeros =     14746 /    147456 ( 10.00%) | total_pruned =    132710 | shape = (128, 128, 3, 3)\n",
      "model.18.bn.weight                       | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.18.bn.bias                         | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.20.cv1.conv.weight                 | nonzeros =      3277 /     32768 ( 10.00%) | total_pruned =     29491 | shape = (128, 256, 1, 1)\n",
      "model.20.cv1.bn.weight                   | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.20.cv1.bn.bias                     | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.20.cv2.conv.weight                 | nonzeros =      3277 /     32768 ( 10.00%) | total_pruned =     29491 | shape = (128, 256, 1, 1)\n",
      "model.20.cv2.bn.weight                   | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.20.cv2.bn.bias                     | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.20.cv3.conv.weight                 | nonzeros =      6554 /     65536 ( 10.00%) | total_pruned =     58982 | shape = (256, 256, 1, 1)\n",
      "model.20.cv3.bn.weight                   | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.20.cv3.bn.bias                     | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.20.m.0.cv1.conv.weight             | nonzeros =      1638 /     16384 ( 10.00%) | total_pruned =     14746 | shape = (128, 128, 1, 1)\n",
      "model.20.m.0.cv1.bn.weight               | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.20.m.0.cv1.bn.bias                 | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.20.m.0.cv2.conv.weight             | nonzeros =     14746 /    147456 ( 10.00%) | total_pruned =    132710 | shape = (128, 128, 3, 3)\n",
      "model.20.m.0.cv2.bn.weight               | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.20.m.0.cv2.bn.bias                 | nonzeros =       128 /       128 (100.00%) | total_pruned =         0 | shape = (128,)\n",
      "model.21.conv.weight                     | nonzeros =     58982 /    589824 ( 10.00%) | total_pruned =    530842 | shape = (256, 256, 3, 3)\n",
      "model.21.bn.weight                       | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.21.bn.bias                         | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.23.cv1.conv.weight                 | nonzeros =     13107 /    131072 ( 10.00%) | total_pruned =    117965 | shape = (256, 512, 1, 1)\n",
      "model.23.cv1.bn.weight                   | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.23.cv1.bn.bias                     | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.23.cv2.conv.weight                 | nonzeros =     13107 /    131072 ( 10.00%) | total_pruned =    117965 | shape = (256, 512, 1, 1)\n",
      "model.23.cv2.bn.weight                   | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.23.cv2.bn.bias                     | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.23.cv3.conv.weight                 | nonzeros =     26214 /    262144 ( 10.00%) | total_pruned =    235930 | shape = (512, 512, 1, 1)\n",
      "model.23.cv3.bn.weight                   | nonzeros =       512 /       512 (100.00%) | total_pruned =         0 | shape = (512,)\n",
      "model.23.cv3.bn.bias                     | nonzeros =       512 /       512 (100.00%) | total_pruned =         0 | shape = (512,)\n",
      "model.23.m.0.cv1.conv.weight             | nonzeros =      6554 /     65536 ( 10.00%) | total_pruned =     58982 | shape = (256, 256, 1, 1)\n",
      "model.23.m.0.cv1.bn.weight               | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.23.m.0.cv1.bn.bias                 | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.23.m.0.cv2.conv.weight             | nonzeros =     58982 /    589824 ( 10.00%) | total_pruned =    530842 | shape = (256, 256, 3, 3)\n",
      "model.23.m.0.cv2.bn.weight               | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.23.m.0.cv2.bn.bias                 | nonzeros =       256 /       256 (100.00%) | total_pruned =         0 | shape = (256,)\n",
      "model.24.m.0.bias                        | nonzeros =       255 /       255 (100.00%) | total_pruned =         0 | shape = (255,)\n",
      "model.24.m.0.weight                      | nonzeros =      3264 /     32640 ( 10.00%) | total_pruned =     29376 | shape = (255, 128, 1, 1)\n",
      "model.24.m.1.bias                        | nonzeros =       255 /       255 (100.00%) | total_pruned =         0 | shape = (255,)\n",
      "model.24.m.1.weight                      | nonzeros =      6528 /     65280 ( 10.00%) | total_pruned =     58752 | shape = (255, 256, 1, 1)\n",
      "model.24.m.2.bias                        | nonzeros =       255 /       255 (100.00%) | total_pruned =         0 | shape = (255,)\n",
      "model.24.m.2.weight                      | nonzeros =     13056 /    130560 ( 10.00%) | total_pruned =    117504 | shape = (255, 512, 1, 1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TOTAL ‚Üí alive: 741,335, pruned: 6,494,054, total: 7,235,389\n",
      "       Compression rate:       9.76x   ( 89.75% pruned)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_nonzeros(model):\n",
    "    nonzero = total = 0\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'mask' in name:\n",
    "            continue  # pruning maskÎäî Î∂ÑÏÑù ÎåÄÏÉÅÏóêÏÑú Ï†úÏô∏\n",
    "        tensor = p.data.cpu().numpy()\n",
    "        nz_count = np.count_nonzero(tensor)\n",
    "        total_params = np.prod(tensor.shape)\n",
    "        nonzero += nz_count\n",
    "        total += total_params\n",
    "        print(f'{name:40} | nonzeros = {nz_count:9} / {total_params:9} '\n",
    "              f'({100 * nz_count / total_params:6.2f}%) | '\n",
    "              f'total_pruned = {total_params - nz_count:9} | shape = {tensor.shape}')\n",
    "    \n",
    "    pruned = total - nonzero\n",
    "    compression = total / nonzero if nonzero > 0 else float('inf')\n",
    "    pruned_percent = 100 * pruned / total\n",
    "    print(\"-\" * 100)\n",
    "    print(f'TOTAL ‚Üí alive: {nonzero:,}, pruned: {pruned:,}, total: {total:,}')\n",
    "    print(f'       Compression rate: {compression:10.2f}x   ({pruned_percent:6.2f}% pruned)')\n",
    "\n",
    "print_nonzeros(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cdc6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hyp_custom.yaml\", \"w\") as f:\n",
    "    f.write(\"\"\"lr0: 0.001\n",
    "lrf: 0.01\n",
    "momentum: 0.937\n",
    "weight_decay: 0.0005\n",
    "warmup_epochs: 3.0\n",
    "warmup_momentum: 0.8\n",
    "warmup_bias_lr: 0.1\n",
    "box: 0.05\n",
    "cls: 0.5\n",
    "cls_pw: 1.0\n",
    "obj: 1.0\n",
    "obj_pw: 1.0\n",
    "iou_t: 0.20\n",
    "anchor_t: 4.0\n",
    "fl_gamma: 0.0\n",
    "hsv_h: 0.015\n",
    "hsv_s: 0.7\n",
    "hsv_v: 0.4\n",
    "degrees: 0.0\n",
    "translate: 0.1\n",
    "scale: 0.5\n",
    "shear: 0.0\n",
    "perspective: 0.0\n",
    "flipud: 0.0\n",
    "fliplr: 0.5\n",
    "mosaic: 1.0\n",
    "mixup: 0.0\n",
    "copy_paste: 0.0\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bfb07ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=runs/train/baseline_yolov5s/weights/pruned_ratio[0.9].pt, cfg=models/yolov5s.yaml, data=data/coco128.yaml, hyp=hyp_custom.yaml, epochs=20, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=retrain_pruned, exist_ok=True, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
      "YOLOv5 üöÄ v7.0-423-g567c6646 Python-3.9.23 torch-2.7.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11172MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/train/baseline_yolov5s/weights/pruned_ratio[0.9].pt\n",
      "/home/yyb02274/yolov5/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "/home/yyb02274/yolov5/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.001) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "WARNING ‚ö†Ô∏è DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\n",
      "See Multi-GPU Tutorial at https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training to get started.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/yyb02274/datasets/coco128/labels/train2017.cache... 126 im\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/yyb02274/datasets/coco128/labels/train2017.cache... 126 imag\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.27 anchors/target, 0.994 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
      "Plotting labels to runs/train/retrain_pruned/labels.jpg... \n",
      "/home/yyb02274/yolov5/train.py:357: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 5 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/retrain_pruned\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]/home/yyb02274/yolov5/train.py:414: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n",
      "       0/19      1.21G    0.08993     0.1252    0.08862        197        640:  /home/yyb02274/yolov5/train.py:414: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n",
      "       0/19      1.41G    0.08924     0.1231    0.09259        187        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929   0.000355    0.00691   0.000389   0.000256\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       1/19      2.79G    0.09134     0.1351     0.0947        191        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929   0.000451    0.00669   0.000555   0.000325\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       2/19       2.8G    0.09167     0.1189    0.09261        201        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929   0.000597     0.0048   0.000523    0.00028\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       3/19      2.82G    0.08803     0.1106    0.09206        129        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929   0.000966    0.00594   0.000834   0.000348\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       4/19      2.84G    0.09163     0.1028    0.09093        185        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929   0.000989    0.00834    0.00104   0.000458\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       5/19      2.84G    0.08923     0.1104    0.09203        229        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929    0.00121     0.0141    0.00124   0.000614\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       6/19      2.84G    0.08936     0.1094    0.08951        188        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929     0.0012     0.0172    0.00101   0.000588\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       7/19      2.84G    0.08903     0.1008    0.08871        210        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929    0.00132     0.0125    0.00148   0.000878\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       8/19      2.84G    0.08876     0.1031    0.08781        246        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929     0.0153     0.0112    0.00831    0.00494\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       9/19      2.84G    0.08847    0.09611    0.08946        197        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929    0.00366    0.00833    0.00277    0.00175\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      10/19      2.84G    0.08648    0.09371     0.0919        185        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929     0.0213     0.0185     0.0135    0.00859\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      11/19      2.84G    0.08521    0.09804    0.09026        258        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929     0.0163     0.0294     0.0138    0.00825\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      12/19      2.84G    0.08405    0.08958    0.08868        291        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929     0.0111     0.0391     0.0126    0.00604\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      13/19      2.84G    0.08497    0.08646    0.08674        213        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929    0.00832     0.0536     0.0125    0.00563\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      14/19      2.84G    0.08497    0.09222    0.08558        197        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929    0.00691     0.0741     0.0126    0.00497\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      15/19      2.84G    0.08363    0.09069    0.08794        262        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929    0.00775       0.11     0.0142    0.00552\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      16/19      2.84G    0.08328    0.09143    0.08249        261        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929    0.00686       0.12      0.015    0.00595\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      17/19      2.84G    0.08394    0.09544    0.08263        273        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929    0.00581      0.116     0.0164    0.00678\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      18/19      2.84G    0.08251    0.08417     0.0871        169        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929    0.00649       0.15     0.0171    0.00685\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      19/19      2.84G    0.08414    0.09476    0.08651        225        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929    0.00605      0.151     0.0159    0.00593\n",
      "\n",
      "20 epochs completed in 0.020 hours.\n",
      "Optimizer stripped from runs/train/retrain_pruned/weights/last.pt, 14.8MB\n",
      "Optimizer stripped from runs/train/retrain_pruned/weights/best.pt, 14.8MB\n",
      "\n",
      "Validating runs/train/retrain_pruned/weights/best.pt...\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929     0.0213     0.0185     0.0135    0.00858\n",
      "                person        128        254     0.0126      0.256      0.019    0.00547\n",
      "               bicycle        128          6          0          0          0          0\n",
      "                   car        128         46          0          0          0          0\n",
      "            motorcycle        128          5          0          0          0          0\n",
      "              airplane        128          6     0.0303      0.333     0.0405     0.0205\n",
      "                   bus        128          7          0          0          0          0\n",
      "                 train        128          3          0          0          0          0\n",
      "                 truck        128         12          0          0          0          0\n",
      "                  boat        128          6          0          0          0          0\n",
      "         traffic light        128         14          0          0          0          0\n",
      "             stop sign        128          2          0          0          0          0\n",
      "                 bench        128          9          0          0          0          0\n",
      "                  bird        128         16          0          0          0          0\n",
      "                   cat        128          4          0          0          0          0\n",
      "                   dog        128          9          0          0          0          0\n",
      "                 horse        128          2          0          0          0          0\n",
      "              elephant        128         17          0          0          0          0\n",
      "                  bear        128          1          0          0          0          0\n",
      "                 zebra        128          4          0          0          0          0\n",
      "               giraffe        128          9          0          0          0          0\n",
      "              backpack        128          6          0          0          0          0\n",
      "              umbrella        128         18          0          0          0          0\n",
      "               handbag        128         19          0          0          0          0\n",
      "                   tie        128          7          0          0          0          0\n",
      "              suitcase        128          4          0          0          0          0\n",
      "               frisbee        128          5          0          0          0          0\n",
      "                  skis        128          1          0          0          0          0\n",
      "             snowboard        128          7          0          0          0          0\n",
      "           sports ball        128          6          0          0          0          0\n",
      "                  kite        128         10          0          0          0          0\n",
      "          baseball bat        128          4          0          0          0          0\n",
      "        baseball glove        128          7          0          0          0          0\n",
      "            skateboard        128          5          0          0          0          0\n",
      "         tennis racket        128          7          0          0          0          0\n",
      "                bottle        128         18          0          0          0          0\n",
      "            wine glass        128         16          0          0          0          0\n",
      "                   cup        128         36          0          0          0          0\n",
      "                  fork        128          6          0          0          0          0\n",
      "                 knife        128         16          0          0          0          0\n",
      "                 spoon        128         22          0          0          0          0\n",
      "                  bowl        128         28          0          0          0          0\n",
      "                banana        128          1          0          0          0          0\n",
      "              sandwich        128          2          0          0          0          0\n",
      "                orange        128          4          0          0          0          0\n",
      "              broccoli        128         11          0          0          0          0\n",
      "                carrot        128         24     0.0488     0.0833      0.034     0.0102\n",
      "               hot dog        128          2          0          0          0          0\n",
      "                 pizza        128          5          0          0          0          0\n",
      "                 donut        128         14          1     0.0714      0.536      0.429\n",
      "                  cake        128          4          0          0          0          0\n",
      "                 chair        128         35          0          0          0          0\n",
      "                 couch        128          6          0          0          0          0\n",
      "          potted plant        128         14          0          0          0          0\n",
      "                   bed        128          3    0.00429      0.333    0.00628    0.00126\n",
      "          dining table        128         13     0.0175     0.0769       0.01      0.002\n",
      "                toilet        128          2          0          0          0          0\n",
      "                    tv        128          2          0          0          0          0\n",
      "                laptop        128          3          0          0          0          0\n",
      "                 mouse        128          2          0          0          0          0\n",
      "                remote        128          8          0          0          0          0\n",
      "            cell phone        128          8          0          0          0          0\n",
      "             microwave        128          3          0          0          0          0\n",
      "                  oven        128          5          0          0          0          0\n",
      "                  sink        128          6          0          0          0          0\n",
      "          refrigerator        128          5          0          0          0          0\n",
      "                  book        128         29          0          0          0          0\n",
      "                 clock        128          9      0.333      0.111      0.262      0.105\n",
      "                  vase        128          2          0          0          0          0\n",
      "              scissors        128          1          0          0          0          0\n",
      "            teddy bear        128         21     0.0625     0.0476     0.0524     0.0367\n",
      "            toothbrush        128          5          0          0          0          0\n",
      "Results saved to \u001b[1mruns/train/retrain_pruned\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "  --weights runs/train/baseline_yolov5s/weights/pruned_ratio[0.9].pt\\\n",
    "  --data data/coco128.yaml \\\n",
    "  --cfg models/yolov5s.yaml \\\n",
    "  --hyp hyp_custom.yaml \\\n",
    "  --epochs 20 \\\n",
    "  --batch-size 16 \\\n",
    "  --name retrain_pruned \\\n",
    "  --exist-ok\n",
    "  #ÍπåÏßÄÎäî Ï†ïÏÉÅÏûëÎèô!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0376fd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[freeze_mask_to_weight] applied to 0 layers.\n"
     ]
    }
   ],
   "source": [
    "def freeze_mask_to_weight(model):\n",
    "    num_applied = 0\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"weight\") and hasattr(m, \"mask\"):\n",
    "            with torch.no_grad():\n",
    "                m.weight.data.mul_(m.mask)\n",
    "            num_applied += 1\n",
    "    print(f\"[freeze_mask_to_weight] applied to {num_applied} layers.\")\n",
    "\n",
    "# pruned_ckpt Î∂àÎü¨Ïò§Í∏∞\n",
    "import torch\n",
    "ckpt = torch.load(\"runs/train/baseline_yolov5s/weights/pruned_ratio[0.9].pt\", map_location=\"cpu\", weights_only=False)\n",
    "model = ckpt['model'] if 'model' in ckpt else ckpt  # YOLOv5 Íµ¨Ï°∞Ïùº Í≤ΩÏö∞ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©\n",
    "\n",
    "freeze_mask_to_weight(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbed8833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved CSR dump to: /home/yyb02274/yolov5/csr_dump_ratio[0.9] | layers: 60\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, json, torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Ï†ÄÏû• ÏúÑÏπò\n",
    "SAVE_DIR = Path(\"csr_dump_ratio[0.9]\")\n",
    "SAVE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# 0Ïù¥ ÏïÑÎãå Í∞íÏúºÎ°ú Í∞ÑÏ£ºÌï† ÏûÑÍ≥ÑÍ∞í (ÏôÑÏ†ÑÌûà 0Ïù¥Î©¥ 0.0, ÌòπÏùÄ 1e-12)\n",
    "THRESH = 0.0\n",
    "\n",
    "def to_csr_2d(mat: np.ndarray, thresh=0.0):\n",
    "    \"\"\"2D numpy array -> CSR(row_ptr, col_ind, values)\"\"\"\n",
    "    rows, cols = mat.shape\n",
    "    row_ptr = [0]\n",
    "    col_ind, values = [], []\n",
    "    nnz = 0\n",
    "    for r in range(rows):\n",
    "        nz_cols = np.nonzero(np.abs(mat[r]) > thresh)[0]\n",
    "        col_ind.extend(nz_cols.tolist())\n",
    "        values.extend(mat[r, nz_cols].tolist())\n",
    "        nnz += len(nz_cols)\n",
    "        row_ptr.append(nnz)\n",
    "    return np.array(row_ptr, np.int64), np.array(col_ind, np.int32), np.array(values, np.float32)\n",
    "\n",
    "def export_model_to_csr(model, save_dir, thresh=0.0):\n",
    "    \"\"\"Conv/Linear Î†àÏù¥Ïñ¥Î•º CSRÎ°ú Ï†ÄÏû•ÌïòÍ≥† manifest.json ÏÉùÏÑ±\"\"\"\n",
    "    manifest = {\"layers\": []}\n",
    "\n",
    "    for name, m in model.named_modules():\n",
    "        if not (hasattr(m, \"weight\") and isinstance(getattr(m, \"weight\", None), torch.Tensor)):\n",
    "            continue\n",
    "\n",
    "        W = m.weight.detach().cpu().numpy()\n",
    "\n",
    "        # Conv\n",
    "        if W.ndim == 4:\n",
    "            O, I, kH, kW = W.shape\n",
    "            W2 = W.reshape(O, I * kH * kW)\n",
    "            shape2d, orig_shape, kind = (O, I*kH*kW), (O, I, kH, kW), \"conv2d\"\n",
    "        # Linear\n",
    "        elif W.ndim == 2:\n",
    "            W2 = W\n",
    "            shape2d, orig_shape, kind = W.shape, W.shape, \"linear\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # CSR Î≥ÄÌôò\n",
    "        rp, ci, val = to_csr_2d(W2, thresh)\n",
    "\n",
    "        # ÏïàÏ†ÑÌïú ÌååÏùº Ïù¥Î¶Ñ\n",
    "        safe_name = name.replace(\".\", \"_\")\n",
    "        np.savez(save_dir / f\"{safe_name}.npz\",\n",
    "                 row_ptr=rp, col_ind=ci, values=val)\n",
    "\n",
    "        # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í∏∞Î°ù\n",
    "        manifest[\"layers\"].append({\n",
    "            \"name\": name,\n",
    "            \"type\": kind,\n",
    "            \"shape\": list(orig_shape),\n",
    "            \"shape2d\": list(shape2d),\n",
    "            \"nnz\": int(val.size),\n",
    "            \"density\": float(val.size) / float(np.prod(shape2d))\n",
    "        })\n",
    "\n",
    "    # manifest Ï†ÄÏû•\n",
    "    with open(save_dir / \"manifest_ratio[0.9].json\", \"w\") as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Saved CSR dump to: {save_dir.resolve()} | layers: {len(manifest['layers'])}\")\n",
    "\n",
    "# Ïã§Ìñâ\n",
    "export_model_to_csr(model, SAVE_DIR, thresh=THRESH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db48206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked layers: 60\n",
      "‚úÖ All matched perfectly (CSR round-trip OK)\n"
     ]
    }
   ],
   "source": [
    "# === Round-trip Í≤ÄÏ¶ù: CSR -> dense Î≥µÏõê == ÏõêÎ≥∏ weight ? ===\n",
    "import json, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "SAVE_DIR = Path(\"csr_dump_ratio[0.9]\")     # CSR Îç§ÌîÑ ÏúÑÏπò\n",
    "THRESH = 0.0                    # ÎÇ¥Î≥¥ÎÇº Îïå ÏçºÎçò ÏûÑÍ≥ÑÍ∞íÍ≥º ÎèôÏùºÌïòÍ≤å\n",
    "MAX_PRINT = 10                  # ÎØ∏Ïä§Îß§Ïπò ÏµúÎåÄ ÌëúÏãú Í∞úÏàò\n",
    "\n",
    "def csr_to_dense(row_ptr, col_ind, values, shape2d):\n",
    "    rows, cols = shape2d\n",
    "    dense = np.zeros((rows, cols), dtype=np.float32)\n",
    "    for r in range(rows):\n",
    "        s, e = int(row_ptr[r]), int(row_ptr[r+1])\n",
    "        cols_r = col_ind[s:e]\n",
    "        vals_r = values[s:e]\n",
    "        dense[r, cols_r] = vals_r\n",
    "    return dense\n",
    "\n",
    "# manifest Î∂àÎü¨Ïò§Í∏∞\n",
    "mani_path = SAVE_DIR / \"manifest_ratio[0.9].json\"\n",
    "assert mani_path.exists(), f\"manifest_ratio[0.9].json not found in {SAVE_DIR}\"\n",
    "manifest = json.load(open(mani_path, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "# modelÏùò Î™®Îìà dict (Ïù¥Î¶Ñ‚ÜíÎ™®Îìà)\n",
    "module_map = dict(model.named_modules())\n",
    "\n",
    "checked, mismatches = 0, []\n",
    "\n",
    "for meta in manifest[\"layers\"]:\n",
    "    name   = meta[\"name\"]\n",
    "    shape2 = tuple(meta[\"shape2d\"])\n",
    "    kind   = meta[\"type\"]\n",
    "\n",
    "    # CSR ÌååÏùº Î°úÎìú (export Ïãú '.' -> '_'Î°ú Ï†ÄÏû•ÌñàÏùå)\n",
    "    npz_path = SAVE_DIR / f\"{name.replace('.', '_')}.npz\"\n",
    "    z = np.load(npz_path)\n",
    "    dense = csr_to_dense(z[\"row_ptr\"], z[\"col_ind\"], z[\"values\"], shape2)\n",
    "\n",
    "    # ÏõêÎ≥∏ Í∞ÄÏ§ëÏπò Í∫ºÎÇ¥ÏÑú 2DÎ°ú Ìé¥Í∏∞\n",
    "    mod = module_map.get(name, None)\n",
    "    if mod is None or not hasattr(mod, \"weight\"):\n",
    "        mismatches.append((name, \"module-not-found\"))\n",
    "        continue\n",
    "\n",
    "    W = mod.weight.detach().cpu().numpy()\n",
    "    if kind == \"conv2d\":\n",
    "        O, I, kH, kW = meta[\"shape\"]\n",
    "        W2 = W.reshape(O, I * kH * kW)\n",
    "    else:\n",
    "        W2 = W  # linear\n",
    "\n",
    "    # ÎπÑÍµê: (1) nnz ÎèôÏùº? (2) Í∞í ÏôÑÏ†Ñ ÎèôÏùº?\n",
    "    nnz_W2 = int(np.count_nonzero(np.abs(W2) > THRESH))\n",
    "    nnz_csr = int(z[\"values\"].size)\n",
    "\n",
    "    same_nnz = (nnz_W2 == nnz_csr)\n",
    "    same_vals = np.allclose(W2, dense, atol=0.0, rtol=0.0)\n",
    "\n",
    "    if not (same_nnz and same_vals):\n",
    "        reason = []\n",
    "        if not same_nnz: reason.append(f\"nnz {nnz_W2} != {nnz_csr}\")\n",
    "        if not same_vals: reason.append(\"values differ\")\n",
    "        mismatches.append((name, \", \".join(reason)))\n",
    "    checked += 1\n",
    "\n",
    "# Í≤∞Í≥º Ï∂úÎ†•\n",
    "print(f\"Checked layers: {checked}\")\n",
    "if mismatches:\n",
    "    print(f\"‚ùå Mismatches: {len(mismatches)} (showing up to {MAX_PRINT})\")\n",
    "    for n, r in mismatches[:MAX_PRINT]:\n",
    "        print(f\" - {n}: {r}\")\n",
    "else:\n",
    "    print(\"‚úÖ All matched perfectly (CSR round-trip OK)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
